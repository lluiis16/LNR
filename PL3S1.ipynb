{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f362cc94",
   "metadata": {},
   "source": [
    "## Practical Work 3\n",
    "### Session 1: Machine Learning Models, and Hyperparameters\n",
    "#### Lluis Pellicer Juan y Jorge De la Cruz Martínez"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00caace",
   "metadata": {},
   "source": [
    "**ACTIVITY**\n",
    "\n",
    "Students must train and validate machine learning models for the \"Oppositional thinking analysis: Conspiracy theories vs. critical thinking narratives\" task. The goal is to distinguish conspiracy narratives from oppositional narratives that do not express a conspiracy mentality (i.e., critical thinking), a binary classification problem. For that purpose:\n",
    "1. Students must train and evaluate at least four classifiers and compare their results. One of the classifiers must be an ensemble of three simple models.\n",
    "2. For each of the three groups of text representation studied in the previous practice (traditional forms, static embedding-based, and contextual embedding-based), students have to choose one representation method and train the selected models using that representation. Preprocess the text as deemed appropriate for each representation method.\n",
    "3. Students must train and evaluate models for both Spanish and English datasets.\n",
    "\n",
    "The proposed models performance must be evaluated based on the official evaluation metric (Matthew correlation coefficient) provided by the task organizers. The official training dataset for both languages are in the file Dataset-Oppositional.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e323abd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn import svm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.tokenize import word_tokenize\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6859496d",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bac8e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ruta de la carpeta donde se encuentran los archivos\n",
    "carpeta_datos = \"C:/Users/lluis/Desktop/lluis/Onedrive/Escritorio/3r Curso/2ndo cuatri/LNR/Dataset-Oppositionl\"\n",
    "\n",
    "# Ruta de los archivos en la carpeta de training\n",
    "ruta_entrenamiento_en = os.path.join(carpeta_datos, \"training\", \"dataset_oppositional\", \"dataset_en_train.json\")\n",
    "ruta_entrenamiento_es = os.path.join(carpeta_datos, \"training\", \"dataset_oppositional\", \"dataset_es_train.json\")\n",
    "\n",
    "# Ruta de los archivos en la carpeta de test\n",
    "ruta_test_en = os.path.join(carpeta_datos, \"test\", \"dataset_oppositional_test_nolabels\", \"dataset_en_official_test_nolabels.json\")\n",
    "ruta_test_es = os.path.join(carpeta_datos, \"test\", \"dataset_oppositional_test_nolabels\", \"dataset_es_official_test_nolabels.json\")\n",
    "\n",
    "# Función para cargar los datos de un archivo JSON\n",
    "def cargar_datos(ruta):\n",
    "    with open(ruta, \"r\", encoding=\"utf-8\") as archivo:\n",
    "        datos = json.load(archivo)\n",
    "    return datos\n",
    "\n",
    "# Cargar datos de los archivos\n",
    "datos_entrenamiento_en = cargar_datos(ruta_entrenamiento_en)\n",
    "datos_entrenamiento_es = cargar_datos(ruta_entrenamiento_es)\n",
    "datos_test_en = cargar_datos(ruta_test_en)\n",
    "datos_test_es = cargar_datos(ruta_test_es)\n",
    "\n",
    "# Extraer textos y etiquetas\n",
    "texts_es_train = [dato[\"text\"] for dato in datos_entrenamiento_es]\n",
    "labels_es_train = [dato[\"category\"] for dato in datos_entrenamiento_es]\n",
    "\n",
    "texts_en_train = [dato[\"text\"] for dato in datos_entrenamiento_en]\n",
    "labels_en_train = [dato[\"category\"] for dato in datos_entrenamiento_en]\n",
    "\n",
    "texts_es_test = [dato[\"text\"] for dato in datos_test_es]\n",
    "texts_en_test = [dato[\"text\"] for dato in datos_test_en]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ba071",
   "metadata": {},
   "source": [
    "## Representation 1: Traditional forms -> CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4b40f4b",
   "metadata": {},
   "source": [
    "### Preprocess and representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10260d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea59cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar textos en inglés\n",
    "preprocessed_texts_en_train = [preprocess_text(text) for text in texts_en_train]\n",
    "preprocessed_texts_en_test = [preprocess_text(text) for text in texts_en_test]\n",
    "\n",
    "# Procesar textos preprocesados usando CountVectorizer para inglés\n",
    "vectorizer_en = CountVectorizer(analyzer='word', max_features=4000, lowercase=True)\n",
    "X_en_train = vectorizer_en.fit_transform(preprocessed_texts_en_train)\n",
    "X_en_test = vectorizer_en.transform(preprocessed_texts_en_test)\n",
    "\n",
    "# Convertir las matrices dispersas a matrices densas\n",
    "X_en_train = X_en_train.toarray()\n",
    "X_en_test = X_en_test.toarray()\n",
    "\n",
    "# Convertir etiquetas a matrices numpy\n",
    "Y_en_train = np.array(labels_en_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47068e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar textos en español\n",
    "preprocessed_texts_es_train = [preprocess_text(text) for text in texts_es_train]\n",
    "preprocessed_texts_es_test = [preprocess_text(text) for text in texts_es_test]\n",
    "\n",
    "# Procesar textos preprocesados usando CountVectorizer para español\n",
    "vectorizer_es = CountVectorizer(analyzer='word', max_features=4000, lowercase=True)\n",
    "X_es_train = vectorizer_es.fit_transform(preprocessed_texts_es_train)\n",
    "X_es_test = vectorizer_es.transform(preprocessed_texts_es_test)\n",
    "\n",
    "# Convertir las matrices dispersas a matrices densas\n",
    "X_es_train = X_es_train.toarray()\n",
    "X_es_test = X_es_test.toarray()\n",
    "\n",
    "# Convertir etiquetas a matrices numpy\n",
    "Y_es_train = np.array(labels_es_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0207e4b9",
   "metadata": {},
   "source": [
    "### Train and evaluation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cea45400",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab68623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define los hiperparámetros que deseas ajustar -> predeterminados son 1 y rbf\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Para el conjunto de datos en inglés\n",
    "X_train_en, X_test_en, y_train_en, y_test_en = train_test_split(X_en_train, Y_en_train, test_size=0.1, random_state=1234)\n",
    "\n",
    "# Inicializa el clasificador SVC\n",
    "clf_en = svm.SVC()\n",
    "\n",
    "# Realiza la búsqueda de cuadrícula\n",
    "grid_search_en = GridSearchCV(estimator=clf_en, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2)\n",
    "grid_search_en.fit(X_train_en, y_train_en)\n",
    "\n",
    "# Muestra los mejores hiperparámetros encontrados\n",
    "print(\"Best Parameters for English:\")\n",
    "print(grid_search_en.best_params_)\n",
    "\n",
    "# Entrena el modelo con los mejores hiperparámetros encontrados\n",
    "clf_en_best = grid_search_en.best_estimator_\n",
    "clf_en_best.fit(X_train_en, y_train_en)\n",
    "\n",
    "# Realiza predicciones en el conjunto de prueba\n",
    "predicted_en = clf_en_best.predict(X_test_en)\n",
    "quality_en = matthews_corrcoef(y_test_en, predicted_en)\n",
    "print(\"MCC for English:\", quality_en)\n",
    "\n",
    "# Para el conjunto de datos en español\n",
    "X_train_es, X_test_es, y_train_es, y_test_es = train_test_split(X_es_train, Y_es_train, test_size=0.1, random_state=1234)\n",
    "\n",
    "# Inicializa el clasificador SVC\n",
    "clf_es = svm.SVC()\n",
    "\n",
    "# Realiza la búsqueda de cuadrícula\n",
    "grid_search_es = GridSearchCV(estimator=clf_es, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2)\n",
    "grid_search_es.fit(X_train_es, y_train_es)\n",
    "\n",
    "# Muestra los mejores hiperparámetros encontrados\n",
    "print(\"Best Parameters for Spanish:\")\n",
    "print(grid_search_es.best_params_)\n",
    "\n",
    "# Entrena el modelo con los mejores hiperparámetros encontrados\n",
    "clf_es_best = grid_search_es.best_estimator_\n",
    "clf_es_best.fit(X_train_es, y_train_es)\n",
    "\n",
    "# Realiza predicciones en el conjunto de prueba\n",
    "predicted_es = clf_es_best.predict(X_test_es)\n",
    "quality_es = matthews_corrcoef(y_test_es, predicted_es)\n",
    "print(\"MCC for Spanish:\", quality_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4415f0e9",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33338474",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Definir los hiperparámetros a probar\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Para el conjunto de datos en inglés\n",
    "grid_search_en = GridSearchCV(LogisticRegression(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search_en.fit(X_train_en, y_train_en)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros para inglés:\", grid_search_en.best_params_)\n",
    "print()\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros para inglés\n",
    "clf_en = grid_search_en.best_estimator_\n",
    "clf_en.fit(X_train_en, y_train_en)\n",
    "predicted_en = clf_en.predict(X_test_en)\n",
    "quality_en = matthews_corrcoef(y_test_en, predicted_en)\n",
    "print(\"MCC for English:\", quality_en)\n",
    "print()\n",
    "\n",
    "# Para el conjunto de datos en español\n",
    "grid_search_es = GridSearchCV(LogisticRegression(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search_es.fit(X_train_es, y_train_es)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros para español:\", grid_search_es.best_params_)\n",
    "print()\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros para español\n",
    "clf_es = grid_search_es.best_estimator_\n",
    "clf_es.fit(X_train_es, y_train_es)\n",
    "predicted_es = clf_es.predict(X_test_es)\n",
    "quality_es = matthews_corrcoef(y_test_es, predicted_es)\n",
    "print(\"MCC for Spanish:\", quality_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e2843b6",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b741d933",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Definir los hiperparámetros a probar\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Para el conjunto de datos en inglés\n",
    "grid_search_en = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search_en.fit(X_train_en, y_train_en)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros para inglés:\", grid_search_en.best_params_)\n",
    "print()\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros para inglés\n",
    "clf_en = grid_search_en.best_estimator_\n",
    "clf_en.fit(X_train_en, y_train_en)\n",
    "predicted_en = clf_en.predict(X_test_en)\n",
    "quality_en = matthews_corrcoef(y_test_en, predicted_en)\n",
    "print(\"MCC for English:\", quality_en)\n",
    "print()\n",
    "\n",
    "# Para el conjunto de datos en español\n",
    "grid_search_es = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search_es.fit(X_train_es, y_train_es)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros para español:\", grid_search_es.best_params_)\n",
    "print()\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros para español\n",
    "clf_es = grid_search_es.best_estimator_\n",
    "clf_es.fit(X_train_es, y_train_es)\n",
    "predicted_es = clf_es.predict(X_test_es)\n",
    "quality_es = matthews_corrcoef(y_test_es, predicted_es)\n",
    "print(\"MCC for Spanish:\", quality_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81c1947",
   "metadata": {},
   "source": [
    "### Ensemble model: Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c070562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores parámetros para SVC\n",
    "svc_params_en = {'C': 0.1, 'kernel': 'linear'}\n",
    "svc_params_es = {'C': 10, 'kernel': 'rbf'}\n",
    "\n",
    "# Mejores parámetros para Logistic Regression\n",
    "lr_params_en = {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "lr_params_es = {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "\n",
    "# Mejores parámetros para Decision Trees\n",
    "dt_params_en = {'criterion': 'gini', 'max_depth': 30, 'min_samples_leaf': 1, 'min_samples_split': 2, 'splitter': 'best'}\n",
    "dt_params_es = {'criterion': 'gini', 'max_depth': 20, 'min_samples_leaf': 4, 'min_samples_split': 10, 'splitter': 'random'}\n",
    "\n",
    "# Crear modelos base para el clasificador de apilamiento\n",
    "base_models_en = [('svc', SVC(**svc_params_en)), ('lr', LogisticRegression(**lr_params_en)), ('dt', DecisionTreeClassifier(**dt_params_en))]\n",
    "base_models_es = [('svc', SVC(**svc_params_es)), ('lr', LogisticRegression(**lr_params_es)), ('dt', DecisionTreeClassifier(**dt_params_es))]\n",
    "\n",
    "# Crear el meta-modelo\n",
    "meta_model_en = LogisticRegression()\n",
    "meta_model_es = LogisticRegression()\n",
    "\n",
    "# Crear el clasificador de apilamiento\n",
    "ensemble_en = StackingClassifier(estimators=base_models_en, final_estimator=meta_model_en)\n",
    "ensemble_es = StackingClassifier(estimators=base_models_es, final_estimator=meta_model_es)\n",
    "\n",
    "# Entrenar el clasificador de apilamiento en los datos de entrenamiento\n",
    "ensemble_en.fit(X_train_en, y_train_en)\n",
    "ensemble_es.fit(X_train_es, y_train_es)\n",
    "\n",
    "# Evaluar el clasificador de apilamiento en los datos de prueba\n",
    "predicted_en = ensemble_en.predict(X_test_en)\n",
    "predicted_es = ensemble_es.predict(X_test_es)\n",
    "\n",
    "# Calcular la calidad de la predicción usando MCC\n",
    "quality_en = matthews_corrcoef(y_test_en, predicted_en)\n",
    "quality_es = matthews_corrcoef(y_test_es, predicted_es)\n",
    "\n",
    "print(\"MCC for English:\", quality_en)\n",
    "print(\"MCC for Spanish:\", quality_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e09c49b",
   "metadata": {},
   "source": [
    "## Representation 2: Static Embedding-Based -> Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec3d724f",
   "metadata": {},
   "source": [
    "###  Preprocess and representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ca5d44",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    preprocessed_text = ' '.join(tokens)\n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22dd398f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar textos en inglés\n",
    "preprocessed_texts_en_train = [preprocess_text(text) for text in texts_en_train]\n",
    "preprocessed_texts_en_test = [preprocess_text(text) for text in texts_en_test]\n",
    "\n",
    "# Combinar todos los textos en una sola lista para entrenar el modelo\n",
    "all_texts_en = preprocessed_texts_en_train + preprocessed_texts_en_test\n",
    "\n",
    "# Tokenizar los textos\n",
    "tokenized_en = [word_tokenize(text.lower()) for text in all_texts_en]\n",
    "\n",
    "# Entrenar el modelo Word2Vec en inglés\n",
    "word2vec_model_en = Word2Vec(sentences=tokenized_en, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Función para obtener las incrustaciones de Word2Vec\n",
    "def get_word2vec_embeddings_en(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            embedding = word2vec_model_en.wv[token]\n",
    "            embeddings.append(embedding)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Aplicar Word2Vec a los textos en inglés\n",
    "X_word2vec_train_en = np.array([get_word2vec_embeddings_en(text) for text in preprocessed_texts_en_train])\n",
    "X_word2vec_test_en = np.array([get_word2vec_embeddings_en(text) for text in preprocessed_texts_en_test])\n",
    "Y_en_train = np.array(labels_en_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49a98a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocesar textos en español\n",
    "preprocessed_texts_es_train = [preprocess_text(text) for text in texts_es_train]\n",
    "preprocessed_texts_es_test = [preprocess_text(text) for text in texts_es_test]\n",
    "\n",
    "# Combinar todos los textos en una sola lista para entrenar el modelo\n",
    "all_texts_es = preprocessed_texts_es_train + preprocessed_texts_es_test\n",
    "\n",
    "# Tokenizar los textos\n",
    "tokenized_es = [word_tokenize(text.lower()) for text in all_texts_es]\n",
    "\n",
    "# Entrenar el modelo Word2Vec en español\n",
    "word2vec_model_es = Word2Vec(sentences=tokenized_es, vector_size=300, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Función para obtener las incrustaciones de Word2Vec\n",
    "def get_word2vec_embeddings_es(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    embeddings = []\n",
    "    for token in tokens:\n",
    "        try:\n",
    "            embedding = word2vec_model_es.wv[token]\n",
    "            embeddings.append(embedding)\n",
    "        except KeyError:\n",
    "            pass\n",
    "    if len(embeddings) > 0:\n",
    "        return np.mean(embeddings, axis=0)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Aplicar Word2Vec a los textos en español\n",
    "X_word2vec_train_es = np.array([get_word2vec_embeddings_es(text) for text in preprocessed_texts_es_train])\n",
    "X_word2vec_test_es = np.array([get_word2vec_embeddings_es(text) for text in preprocessed_texts_es_test])\n",
    "Y_es_train = np.array(labels_es_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb60535",
   "metadata": {},
   "source": [
    "### Train and evaluation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c150bd",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1795d9a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los parámetros a probar\n",
    "param_grid = {\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'C': [0.1, 1, 10, 100]\n",
    "}\n",
    "\n",
    "# Para el conjunto de datos en inglés\n",
    "X_train_word2vec_en, X_test_word2vec_en, y_train_word2vec_en, y_test_word2vec_en = train_test_split(X_word2vec_train_en, Y_en_train, test_size=0.1, random_state=1234)\n",
    "\n",
    "# Inicializa el clasificador SVC\n",
    "clf_en = svm.SVC()\n",
    "\n",
    "# Realiza la búsqueda de cuadrícula\n",
    "grid_search_en = GridSearchCV(estimator=clf_en, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2)\n",
    "grid_search_en.fit(X_train_word2vec_en, y_train_word2vec_en)\n",
    "\n",
    "# Muestra los mejores hiperparámetros encontrados\n",
    "print(\"Best Parameters for English:\")\n",
    "print(grid_search_en.best_params_)\n",
    "\n",
    "# Entrena el modelo con los mejores hiperparámetros encontrados\n",
    "clf_en_best = grid_search_en.best_estimator_\n",
    "clf_en_best.fit(X_train_word2vec_en, y_train_word2vec_en)\n",
    "\n",
    "# Realiza predicciones en el conjunto de prueba\n",
    "predicted_word2vec_en = clf_en_best.predict(X_test_word2vec_en)\n",
    "quality_en = matthews_corrcoef(y_test_word2vec_en, predicted_word2vec_en)\n",
    "print(\"MCC for English:\", quality_en)\n",
    "\n",
    "\n",
    "# Para el conjunto de datos en español\n",
    "X_train_word2vec_es, X_test_word2vec_es, y_train_word2vec_es, y_test_word2vec_es = train_test_split(X_word2vec_train_es, Y_es_train, test_size=0.1, random_state=1234)\n",
    "\n",
    "# Inicializa el clasificador SVC\n",
    "clf_es = svm.SVC()\n",
    "\n",
    "# Realiza la búsqueda de cuadrícula\n",
    "grid_search_es = GridSearchCV(estimator=clf_es, param_grid=param_grid, cv=3, scoring='accuracy', verbose=2)\n",
    "grid_search_es.fit(X_train_word2vec_es, y_train_word2vec_es)\n",
    "\n",
    "# Muestra los mejores hiperparámetros encontrados\n",
    "print(\"Best Parameters for Spanish:\")\n",
    "print(grid_search_es.best_params_)\n",
    "\n",
    "# Entrena el modelo con los mejores hiperparámetros encontrados\n",
    "clf_es_best = grid_search_es.best_estimator_\n",
    "clf_es_best.fit(X_train_word2vec_es, y_train_word2vec_es)\n",
    "\n",
    "# Realiza predicciones en el conjunto de prueba\n",
    "predicted_word2vec_es = clf_es_best.predict(X_test_word2vec_es)\n",
    "quality_es = matthews_corrcoef(y_test_word2vec_es, predicted_word2vec_es)\n",
    "print(\"MCC for Spanish:\", quality_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b863852",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a18a9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los hiperparámetros a probar\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "# Para el conjunto de datos en inglés\n",
    "grid_search_en = GridSearchCV(LogisticRegression(), param_grid, cv=2, scoring='accuracy')\n",
    "grid_search_en.fit(X_train_word2vec_en, y_train_word2vec_en)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros para inglés:\", grid_search_en.best_params_)\n",
    "print()\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros para inglés\n",
    "clf_en = grid_search_en.best_estimator_\n",
    "clf_en.fit(X_train_word2vec_en, y_train_word2vec_en)\n",
    "predicted_en = clf_en.predict(X_test_word2vec_en)\n",
    "quality_en = matthews_corrcoef(y_test_word2vec_en, predicted_en)\n",
    "print(\"MCC for English:\", quality_en)\n",
    "print()\n",
    "\n",
    "# Para el conjunto de datos en español\n",
    "grid_search_es = GridSearchCV(LogisticRegression(), param_grid, cv=2, scoring='accuracy')\n",
    "grid_search_es.fit(X_train_word2vec_es, y_train_word2vec_es)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros para español:\", grid_search_es.best_params_)\n",
    "print()\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros para español\n",
    "clf_es = grid_search_es.best_estimator_\n",
    "clf_es.fit(X_train_word2vec_es, y_train_word2vec_es)\n",
    "predicted_es = clf_es.predict(X_test_word2vec_es)\n",
    "quality_es = matthews_corrcoef(y_test_word2vec_es, predicted_es)\n",
    "print(\"MCC for Spanish:\", quality_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0329ee2",
   "metadata": {},
   "source": [
    "### Decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd84065b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los hiperparámetros a probar\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Para el conjunto de datos en inglés\n",
    "grid_search_en = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search_en.fit(X_train_word2vec_en, y_train_word2vec_en)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros para inglés:\", grid_search_en.best_params_)\n",
    "print()\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros para inglés\n",
    "clf_en = grid_search_en.best_estimator_\n",
    "clf_en.fit(X_train_word2vec_en, y_train_word2vec_en)\n",
    "predicted_en = clf_en.predict(X_test_word2vec_en)\n",
    "quality_en = matthews_corrcoef(y_test_word2vec_en, predicted_en)\n",
    "print(\"MCC for English:\", quality_en)\n",
    "print()\n",
    "\n",
    "# Para el conjunto de datos en español\n",
    "grid_search_es = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search_es.fit(X_train_word2vec_es, y_train_word2vec_es)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros para español:\", grid_search_es.best_params_)\n",
    "print()\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros para español\n",
    "clf_es = grid_search_es.best_estimator_\n",
    "clf_es.fit(X_train_word2vec_es, y_train_word2vec_es)\n",
    "predicted_es = clf_es.predict(X_test_word2vec_es)\n",
    "quality_es = matthews_corrcoef(y_test_word2vec_es, predicted_es)\n",
    "print(\"MCC for Spanish:\", quality_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed0278a",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd08472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores parámetros para SVC\n",
    "svc_params_en = {'C': 100, 'kernel': 'linear'}\n",
    "svc_params_es = {'C': 100, 'kernel': 'linear'}\n",
    "\n",
    "# Mejores parámetros para Logistic Regression\n",
    "lr_params_en = {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "lr_params_es = {'C': 10, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "\n",
    "# Mejores parámetros para Decision Trees\n",
    "dt_params_en = {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 2, 'splitter': 'random'}\n",
    "dt_params_es = {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 10, 'splitter': 'best'}\n",
    "\n",
    "# Crear modelos base para el clasificador de apilamiento\n",
    "base_models_en = [('svc', SVC(**svc_params_en)), ('lr', LogisticRegression(**lr_params_en)), ('dt', DecisionTreeClassifier(**dt_params_en))]\n",
    "base_models_es = [('svc', SVC(**svc_params_es)), ('lr', LogisticRegression(**lr_params_es)), ('dt', DecisionTreeClassifier(**dt_params_es))]\n",
    "\n",
    "# Crear el meta-modelo\n",
    "meta_model_en = LogisticRegression()\n",
    "meta_model_es = LogisticRegression()\n",
    "\n",
    "# Crear el clasificador de apilamiento\n",
    "ensemble_en = StackingClassifier(estimators=base_models_en, final_estimator=meta_model_en)\n",
    "ensemble_es = StackingClassifier(estimators=base_models_es, final_estimator=meta_model_es)\n",
    "\n",
    "# Entrenar el clasificador de apilamiento en los datos de entrenamiento\n",
    "ensemble_en.fit(X_train_word2vec_en, y_train_word2vec_en)\n",
    "ensemble_es.fit(X_train_word2vec_es, y_train_word2vec_es)\n",
    "\n",
    "# Evaluar el clasificador de apilamiento en los datos de prueba\n",
    "predicted_en = ensemble_en.predict(X_test_word2vec_en)\n",
    "predicted_es = ensemble_es.predict(X_test_word2vec_es)\n",
    "\n",
    "# Calcular la calidad de la predicción usando MCC\n",
    "quality_en = matthews_corrcoef(y_test_word2vec_en, predicted_en)\n",
    "quality_es = matthews_corrcoef(y_test_word2vec_es, predicted_es)\n",
    "\n",
    "print(\"MCC for English:\", quality_en)\n",
    "print(\"MCC for Spanish:\", quality_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "970909b0",
   "metadata": {},
   "source": [
    "## Representation 3: Contextual Embedding-Based -> BertModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c23d87",
   "metadata": {},
   "source": [
    "### Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfeb8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el tokenizador y el modelo BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Función para obtener las incrustaciones contextuales con BERT\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embeddings = outputs.last_hidden_state[:, 0, :]  # CLS token embeddings\n",
    "    return cls_embeddings.numpy()\n",
    "\n",
    "# Aplicar BERT a los textos en inglés\n",
    "X_train_bert_en = np.array([get_bert_embeddings(text) for text in texts_en_train])\n",
    "# Aplicar BERT a los textos en español\n",
    "X_train_bert_es = np.array([get_bert_embeddings(text) for text in texts_es_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c6be3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el conjunto de datos en inglés con incrustaciones BERT\n",
    "X_train_bert_en, X_test_bert_en, y_train_bert_en, y_test_bert_en = train_test_split(X_train_bert_en, labels_en_train, test_size=0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3873466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el conjunto de datos en inglés con incrustaciones BERT\n",
    "X_train_bert_es, X_test_bert_es, y_train_bert_es, y_test_bert_es = train_test_split(X_train_bert_es, labels_es_train, test_size=0.1, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf136f9",
   "metadata": {},
   "source": [
    "### Train and Evaluation of models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b75a2ddd",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdf522b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los parámetros a probar\n",
    "param_grid_svc = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10],\n",
    "    'kernel': ['linear', 'rbf', 'poly']\n",
    "}\n",
    "\n",
    "X_train_bert_en_flat = X_train_bert_en.reshape(X_train_bert_en.shape[0], -1)\n",
    "X_train_bert_es_flat = X_train_bert_es.reshape(X_train_bert_es.shape[0], -1)\n",
    "\n",
    "# Para el conjunto de datos en inglés\n",
    "grid_search_svc_en = GridSearchCV(SVC(), param_grid_svc, cv=3, scoring='accuracy')\n",
    "grid_search_svc_en.fit(X_train_bert_en_flat, y_train_bert_en)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados para inglés\n",
    "print(\"Mejores hiperparámetros para inglés:\", grid_search_svc_en.best_params_)\n",
    "\n",
    "# Mejores parámetros para SVC en inglés\n",
    "best_params_svc_en = grid_search_svc_en.best_params_\n",
    "\n",
    "# Inicializar y ajustar el modelo SVC en inglés con los mejores hiperparámetros\n",
    "svc_en = SVC(**best_params_svc_en)\n",
    "svc_en.fit(X_train_bert_en_flat, y_train_bert_en)\n",
    "\n",
    "# Aplanar los datos de prueba en inglés\n",
    "X_test_bert_en_flat = X_test_bert_en.reshape(X_test_bert_en.shape[0], -1)\n",
    "\n",
    "# Evaluar el modelo en inglés\n",
    "predicted_en = svc_en.predict(X_test_bert_en_flat)\n",
    "quality_en = matthews_corrcoef(y_test_bert_en, predicted_en)\n",
    "print(\"MCC for English:\", quality_en)\n",
    "\n",
    "# Para el conjunto de datos en español\n",
    "grid_search_svc_es = GridSearchCV(SVC(), param_grid_svc, cv=3, scoring='accuracy')\n",
    "grid_search_svc_es.fit(X_train_bert_es_flat, y_train_bert_es)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados para español\n",
    "print(\"Mejores hiperparámetros para español:\", grid_search_svc_es.best_params_)\n",
    "\n",
    "# Mejores parámetros para SVC en español\n",
    "best_params_svc_es = grid_search_svc_es.best_params_\n",
    "\n",
    "# Inicializar y ajustar el modelo SVC en español con los mejores hiperparámetros\n",
    "svc_es = SVC(**best_params_svc_es)\n",
    "svc_es.fit(X_train_bert_es_flat, y_train_bert_es)\n",
    "\n",
    "# Aplanar los datos de prueba en español\n",
    "X_test_bert_es_flat = X_test_bert_es.reshape(X_test_bert_es.shape[0], -1)\n",
    "\n",
    "# Evaluar el modelo en español\n",
    "predicted_es = svc_es.predict(X_test_bert_es_flat)\n",
    "quality_es = matthews_corrcoef(y_test_bert_es, predicted_es)\n",
    "print(\"MCC for Spanish:\", quality_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40edd70e",
   "metadata": {},
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426c324d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los hiperparámetros a probar\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'newton-cg', 'lbfgs', 'sag', 'saga']\n",
    "}\n",
    "\n",
    "X_train_bert_en_flat = X_train_bert_en.reshape(X_train_bert_en.shape[0], -1)\n",
    "X_train_bert_es_flat = X_train_bert_es.reshape(X_train_bert_es.shape[0], -1)\n",
    "\n",
    "# Para el conjunto de datos en inglés con incrustaciones BERT\n",
    "grid_search_en = GridSearchCV(LogisticRegression(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search_en.fit(X_train_bert_en_flat, y_train_bert_en)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros para inglés:\", grid_search_en.best_params_)\n",
    "print()\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros para inglés\n",
    "clf_en = grid_search_en.best_estimator_\n",
    "clf_en.fit(X_train_bert_en_flat, y_train_bert_en)\n",
    "\n",
    "# Aplanar los datos de prueba en inglés\n",
    "X_test_bert_en_flat = X_test_bert_en.reshape(X_test_bert_en.shape[0], -1)\n",
    "\n",
    "predicted_en = clf_en.predict(X_test_bert_en_flat)\n",
    "quality_en = matthews_corrcoef(y_test_bert_en, predicted_en)\n",
    "print(\"MCC for English:\", quality_en)\n",
    "print()\n",
    "\n",
    "# Para el conjunto de datos en español con incrustaciones BERT\n",
    "grid_search_es = GridSearchCV(LogisticRegression(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search_es.fit(X_train_bert_es_flat, y_train_bert_es)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros para español:\", grid_search_es.best_params_)\n",
    "print()\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros para español\n",
    "clf_es = grid_search_es.best_estimator_\n",
    "clf_es.fit(X_train_bert_es_flat, y_train_bert_es)\n",
    "X_test_bert_es_flat = X_test_bert_es.reshape(X_test_bert_es.shape[0], -1)\n",
    "predicted_es = clf_es.predict(X_test_bert_es_flat)\n",
    "quality_es = matthews_corrcoef(y_test_bert_es, predicted_es)\n",
    "print(\"MCC for Spanish:\", quality_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eedee30",
   "metadata": {},
   "source": [
    "### Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9333880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los hiperparámetros a probar\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'splitter': ['best', 'random'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "X_train_bert_en_flat = X_train_bert_en.reshape(X_train_bert_en.shape[0], -1)\n",
    "X_train_bert_es_flat = X_train_bert_es.reshape(X_train_bert_es.shape[0], -1)\n",
    "\n",
    "# Para el conjunto de datos en inglés con incrustaciones BERT\n",
    "grid_search_en = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search_en.fit(X_train_bert_en_flat, y_train_bert_en)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros para inglés:\", grid_search_en.best_params_)\n",
    "print()\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros para inglés\n",
    "clf_en = grid_search_en.best_estimator_\n",
    "clf_en.fit(X_train_bert_en_flat, y_train_bert_en)\n",
    "\n",
    "# Aplanar los datos de prueba en inglés\n",
    "X_test_bert_en_flat = X_test_bert_en.reshape(X_test_bert_en.shape[0], -1)\n",
    "\n",
    "predicted_en = clf_en.predict(X_test_bert_en_flat)\n",
    "quality_en = matthews_corrcoef(y_test_bert_en, predicted_en)\n",
    "print(\"MCC for English:\", quality_en)\n",
    "print()\n",
    "\n",
    "# Para el conjunto de datos en español con incrustaciones BERT\n",
    "grid_search_es = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=3, scoring='accuracy')\n",
    "grid_search_es.fit(X_train_bert_es_flat, y_train_bert_es)\n",
    "\n",
    "# Imprimir los mejores hiperparámetros encontrados\n",
    "print(\"Mejores hiperparámetros para español:\", grid_search_es.best_params_)\n",
    "print()\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros para español\n",
    "clf_es = grid_search_es.best_estimator_\n",
    "clf_es.fit(X_train_bert_es_flat, y_train_bert_es)\n",
    "X_test_bert_es_flat = X_test_bert_es.reshape(X_test_bert_es.shape[0], -1)\n",
    "predicted_es = clf_es.predict(X_test_bert_es_flat)\n",
    "quality_es = matthews_corrcoef(y_test_bert_es, predicted_es)\n",
    "print(\"MCC for Spanish:\", quality_es)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45dc260c",
   "metadata": {},
   "source": [
    "### Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2defd477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores parámetros para SVC\n",
    "svc_params_en = {'C': 10, 'kernel': 'rbf'}\n",
    "svc_params_es = {'C': 10, 'kernel': 'rbf'}\n",
    "\n",
    "# Mejores parámetros para Logistic Regression\n",
    "lr_params_en = {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "lr_params_es = {'C': 0.01, 'penalty': 'l1', 'solver': 'liblinear'}\n",
    "\n",
    "# Mejores parámetros para Decision Trees\n",
    "dt_params_en = {'criterion': 'gini', 'max_depth': 10, 'min_samples_leaf': 1, 'min_samples_split': 5, 'splitter': 'best'}\n",
    "dt_params_es = {'criterion': 'entropy', 'max_depth': 10, 'min_samples_leaf': 2, 'min_samples_split': 5, 'splitter': 'best'}\n",
    "\n",
    "# Crear modelos base para el clasificador de apilamiento\n",
    "base_models_en = [('svc', SVC(**svc_params_en)), ('lr', LogisticRegression(**lr_params_en)), ('dt', DecisionTreeClassifier(**dt_params_en))]\n",
    "base_models_es = [('svc', SVC(**svc_params_es)), ('lr', LogisticRegression(**lr_params_es)), ('dt', DecisionTreeClassifier(**dt_params_es))]\n",
    "\n",
    "# Crear el meta-modelo\n",
    "meta_model_en = LogisticRegression()\n",
    "meta_model_es = LogisticRegression()\n",
    "\n",
    "X_train_bert_en_flat = X_train_bert_en.reshape(X_train_bert_en.shape[0], -1)\n",
    "X_train_bert_es_flat = X_train_bert_es.reshape(X_train_bert_es.shape[0], -1)\n",
    "X_test_bert_en_flat = X_test_bert_en.reshape(X_test_bert_en.shape[0], -1)\n",
    "X_test_bert_es_flat = X_test_bert_es.reshape(X_test_bert_es.shape[0], -1)\n",
    "\n",
    "# Crear el clasificador de apilamiento\n",
    "ensemble_en = StackingClassifier(estimators=base_models_en, final_estimator=meta_model_en)\n",
    "ensemble_es = StackingClassifier(estimators=base_models_es, final_estimator=meta_model_es)\n",
    "\n",
    "# Entrenar el clasificador de apilamiento en los datos de entrenamiento\n",
    "ensemble_en.fit(X_train_bert_en_flat, y_train_bert_en)\n",
    "ensemble_es.fit(X_train_bert_es_flat, y_train_bert_es)\n",
    "\n",
    "# Evaluar el clasificador de apilamiento en los datos de prueba\n",
    "predicted_en = ensemble_en.predict(X_test_bert_en_flat)\n",
    "predicted_es = ensemble_es.predict(X_test_bert_es_flat)\n",
    "\n",
    "# Calcular la calidad de la predicción usando MCC\n",
    "quality_en = matthews_corrcoef(y_test_bert_en, predicted_en)\n",
    "quality_es = matthews_corrcoef(y_test_bert_es, predicted_es)\n",
    "\n",
    "print(\"MCC for English:\", quality_en)\n",
    "print(\"MCC for Spanish:\", quality_es)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
